{
    "file_name": "unknown",
    "file_path": "",
    "chunks": [
        {
            "cluster": 2,
            "chunk_content": "What is a cluster?1.1. Connecting to the cluster2.1. Connect to the cluster3. What is a cluster?1.1. Connecting to the cluster2.1. Connect to the cluster\n2.1. Connect to the cluster\n3. Nowadays, however, the majority of supercomputers are instead computer clusters (or just \"clusters\" for short) --- collections of relatively low-cost standalone computers that are networked together. Each computer in the cluster is called a node (the term \"node\" comes from graph theory), and we commonly talk about two types of nodes: head node and compute nodes. Head Node - The head node is the computer where we land when we log in to the cluster. Head Node - The head node is the computer where we land when we log in to the cluster. A basic structure can be understood by the following diagram:-\n1. What is a cluster? A cluster consists of a large number of interconnected machines. The nodes running your programs are usually called compute nodes. To turn the nodes into a cluster we need three more ingredients: a network, data storage, and a queue manager. In section we’ll discuss these components in a bit more detail to give you an understanding of how a cluster works. Network\nNodes in the cluster connect to a high-performance network. The network allows programs running on different nodes to “talk” to each other. Things to remember…\nA few things that you should keep in mind when using the cluster:\nIn the coming sections you will learn how to connect to the cluster so that you can start submitting jobs. The nodes in a cluster are set up so that they’re all (more or less) identical. The nodes in a cluster are set up so that they’re all (more or less) identical. You access the cluster through a single node, often denoted the frontend or head node (in our case hpclogin). The frontend node is identical to the other nodes, but it is set up to allow access from the Internet. Your day-to-day interaction with the cluster goes through the frontend. You access the cluster through a single node, often denoted the frontend or head node (in our case hpclogin). The frontend node is identical to the other nodes, but it is set up to allow access from the Internet. Your day-to-day interaction with the cluster goes through the frontend. Connecting to the cluster\nBelow you can find a few basic steps, to get you setup and ready to connect to IITJ HPC. Connect to the cluster\nWhen you receive confirmation, that your account has been activated, you can then connect to the cluster by opening a terminal and using:\nssh user-name@hpclogin.iitj.ac.inIf you need to display any graphics, you can use instead\nssh -X user-name@hpclogin.iitj.ac.inAnd a session on the terminal will be opened, within the login node of the cluster."
        },
        {
            "cluster": 4,
            "chunk_content": "Learning to use the shell2. Learning to use the shell\n1.1. Things to remember…\n1.5. Learning to use the shell\n2. Learning to use the shell\nWhen interacting with the cluster you will be using a shell on a Linux/UNIX system. If you are not familiar with these concepts we recommend the Shell novice tutorial fromSoftware Carpentry\n2. Terminal\nIf you are on a UNIX system (i.e. either Linux or Mac OS), a terminal will already be present natively in your operating system. If you are on Windows, then we recommend you install Putty or MobaXTerm which is a very good UNIX emulator, and includes already useful things like an X server and a package management system to help you use languages like Python or Perl. Users that are familiar with Sun Grid Engine (SGE) or Portable Batch System (PBS), will find Slurm very familiar. In the following sections we the method. Useful module commands\nmodule avail: display the available packages that can be loaded\nmodule list: lists the loaded packages\nmodule load foo: to load the package foo\nmodule rm foo: to unload the package foo\nmodule purge: to unload all the loaded packages\nList of available Modules is below:-\nFor detailed information on the usage of module check the man pages$ man module"
        },
        {
            "cluster": 3,
            "chunk_content": "Running Jobs on the HPC3.1. Running Jobs on the HPC3.1. The head nodes are shared with other users and jobs should not be run on the head nodes themselves. The head nodes are shared with other users and jobs should not be run on the head nodes themselves. Compute Node - The compute nodes are the computers where jobs should be run. In order to run jobs on the compute nodes we must go through the job scheduler. By submitting jobs to the job scheduler, the jobs will automatically be run on the compute nodes once the requested resources are available. Compute Node - The compute nodes are the computers where jobs should be run. In order to run jobs on the compute nodes we must go through the job scheduler. By submitting jobs to the job scheduler, the jobs will automatically be run on the compute nodes once the requested resources are available. Queue manager\nIn theory, any user would be able to log in to any node and run a program, which may consume the resources of the entire node. Alternatively, one user could start thousands of runs of a program on different nodes, consuming the resources of the entire cluster for an unknown duration of time. To solve this problem we need a queue manager, which is a software that manages the requests for resources in a fair way. Like at the supermarket or an office, you stand in the queue and wait until it’s your turn. On the cluster, you submit jobs to the queue and your jobs will run on some node chosen by the queue manager, once resources are available. The queue manager also allows you to specify certain requirements for your program to run. If you specify this when submitting your job, the queue manager will make sure to run the job on a node with at least that amount of memory available. At IITJ we use a queue manager called Slurm: to run your programs on the HPC, you’ll be interacting a lot with Slurm. When the queue manager runs your job on a node, it more or less corresponds to you logging in to the node and running the program yourself. When the queue manager runs your job on a node, it more or less corresponds to you logging in to the node and running the program yourself. Running Jobs on the HPC\nSince IITJ HPC is a shared system, all computations must be carried out through a queue. Users submit jobs to the queue and the jobs then run when it’s their turn. To handle different workloads, jobs can be submitted to one or more partitions, which are essentially queues that have been assigned certain restrictions such as the maximum running time. The queueing system used at IITJ is Slurm. The queueing system allows us to submit an batch job. Batch jobs\nBatch jobs are best suited for computations lasting longer or resource demanding analyses."
        },
        {
            "cluster": 1,
            "chunk_content": "Cancelling a job\n3.1. Checking job status\n3.3. Cancelling a job\n4. This is where we edit scripts, compile code, and submit jobs to the scheduler. This is where we edit scripts, compile code, and submit jobs to the scheduler. Batch jobs are submitted to the queue like interactive jobs, but they don’t give you a shell to run commands. Instead, you must write a job script which contains the commands that needs to be run. A job script looks like this:\n#!/bin/bash#SBATCH --partition small#SBATCH -D /my/working/directory#SBATCH -c 1#SBATCH --mem 4G#SBATCH -t 02:00:00\necho hello world > result.txt\nThe lines beginning with #SBATCH communicate to Slurm which parameters we want to set for a specific task, or what kind of resources we want to be reserved for that particular analysis. The job script specifies which resources are needed as well as the commands to be run. Line 2 specifies that this job should be submitted to the partition. Line 3 tells the scheduler that on execution should go (or chdir) to a specific folder where we want the analysis to be performed. Finally, line 5 indicates we are reserving 2 hours to execute this script. Warning\nAt the moment (issue under investigation) all three of –mem, -t and -D need to be specified in the job script, to make sure the job is scheduled in the correct way and your work is distributed as much as possible across all available resources. See the table below for an overview of commonly used resource flags:\nThe rest of the script is a normal Bash script which contains the commands that should be executed, when the job is started by Slurm. To submit a job for this script, save it to a file (e.g. example.sh) and run:\n[myuser@headnode1]$ sbatch example.shSubmitted batch job 17129500[myuser@headnode1]$\n3.2. Checking job status\nTo check the status of a job:[myuser@headnode1]$ squeue -j 17129500To check the status of all of your submitted jobs:[myuser@headnode1]$ squeue -u USERNAMEYou can also omit the username flag to get an overview of all jobs that have been submitted to the queue:[myuser@headnode1]$ squeue\n3.3. Cancelling a job\nJobs can be cancelled using the scancel command:[myuser@headnode1]$ scancel 17129500\n4. In-order to use python3.7 the appropriate module should be loaded. $module load python/3.7\nModules are “configuration” set/change/remove environment variables from the current environment."
        },
        {
            "cluster": 0,
            "chunk_content": "Applications\n»\nHome\n\nHigh Performance Computing\nBack in the day, the machines used for high performance computing were known as \"supercomputers\" or big standalone machines with specialized hardware–very different from what you would find in home and office computers. These inter-connected computers are endowed with software to coordinate programs on (or across) those computers, and they can therefore work together to perform computationally intensive tasks. Each machine is very much like your own desktop computer, except it’s much more powerful. Like your desktop computer, each machine (often referred to as node) has a CPU, some memory, and a hard disk. Storage\nThe process is data intensive so we can’t just use a single hard disk as you would in your own computer. Instead, we buy several dedicated hard disks and use a file system that can manage files across these disks. The storage might be divided into partitions, and presented to the user in different locations on their path. This could cause other users’ programs on the same node to crash. In short, it would be impossible to share the HPC without a system to handle requests. For example, your program may need to run on a node with a lot of memory. Software that is available on one node will also be available on all other nodes and you can access your files in the same way on all nodes. Software that is available on one node will also be available on all other nodes and you can access your files in the same way on all nodes. But, you should not run any computation or memory intensive programs on the frontend. All users share the frontend’s resources and you should only use it for basics things like looking around the file system, writing scripts, and submitting jobs. But, you should not run any computation or memory intensive programs on the frontend. All users share the frontend’s resources and you should only use it for basics things like looking around the file system, writing scripts, and submitting jobs. Note :\nA node can be shared by multiple users, so you should always take extra care in requesting to correct amount of resources (nodes, cores and memory). There is no reason to occupy an entire node if you are only using a single core and a few gigabytes of memory. Always make sure to use the resources on the requested nodes efficiently. To get an overview of the available partitions:\n[myuser@headnode1]$ sinfo\nThis will list each partition and all of the compute nodes assigned to each partition and the maximum walltime (running time) a job in the partition can have. By typing a slightly different command:\n[myuser@headnode1]$ sinfo -N --long\nAll nodes will be listed and all partitions they belong to, together with the available resources such as the number of cores per node, available memory per node. Line 4 specifies that we want a single core to run on, and line 5 specifies that we want 4G of memory per allocated core. Applications\nSeveral programs with different versions are available on HPC systems. Having all the versions at the disposal of the user simultaneously leads to library conflicts and clashes. In any production environment only the needed packages should be included in the environment of the user."
        }
    ]
}