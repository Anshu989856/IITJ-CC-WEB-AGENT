{
    "file_name": "ocis_7.0_deployment_container_orchestration_orchestration.html",
    "file_path": "./owncloud_docs\\ocis_7.0_deployment_container_orchestration_orchestration.html",
    "chunks": [
        {
            "cluster": 2,
            "chunk_content": "Documentation for ownCloud (A Kiteworks Company)\nInfinite Scale Documentation\nIntroduction\nQuick Guide\nInfinite Scale Overview\nArchitecture and Concepts\nAvailability and Scalability\nSecurity Aspects\nDeployment\nPrerequisites\nStorage\nGeneral Storage Considerations\nNetwork File System\nS3\nPosixFS\nGeneral Information\nThe ocis init Command\nContainer Orchestration\nOffice Applications using WOPI\nSecuring oCIS\nUser Triggered GDPR Report\nServices\nGeneral Information\nDeployment Considerations\nEnvironment Variable Notes\nEnvironment Variables with Special Scope\nEnvironment Variable Changes\nRegistry\nCaching and Persistence\nGateways\nLogging\nPorts Used\nTransport Layer Security\nList of Services\nActivitylog\nAntivirus\nApp Provider\nApp Registry\nAudit\nAuth App\nAuth Basic\nAuth Bearer\nAuth Machine\nAuth Service\nClientlog\nCollaboration\nEventhistory\nFrontend\nGateway\nGraph\nGroups\nIDM\nInvitations\nIDP\nNATS\nNotifications\nOCDAV\nOCM\nOCS\nPolicies\nPostprocessing\nProxy\nSearch\nSettings\nSharing\nSSE\nStore\nStorage-Publiclink\nStorage-Shares\nStorage-System\nStorage-Users\nThumbnails\nUserlog\nUsers\nWeb\nWebDAV\nWebfinger\nWeb UI\nCustom Configuration\nCustom Theming\nMaintenance\nBackup and Restore\nBackup Considerations\nBackup\nRestore\nMaintenance Commands\nListing Space IDs\nListing User IDs\nMigration and Upgrades\nUpgrading Infinite Scale\nMonitoring\nPrometheus\nConfiguration Examples\nOffice Integration\nSearch\nDeployment Examples\nMinimal Bare Metal\nBare Metal with systemd\nContainer Setup\nUbuntu with Docker Compose\nLocal Production Setup\nDeployment on Hetzner\nSetup Federations\nScienceMesh Setup\nAdditional Information\nKnowledge Base\nInfinite Scale Documentation\n7.0\nDesktop App\nnext\n5.3\n5.2\nInfinite Scale Documentation\nnext\n7.1\n7.0\nMobile App for Android\nnext\n4.5\n4.4\nMobile App for iOS\nnext\n12.4\n12.3\nownCloud Main Page\nownCloud Server\nnext\n10.15\n10.14\nownCloud Web User Interfaces\nnext\nInfinite Scale Documentation\nDeployment\nContainer Orchestration\nnext\n7.1\n7.0\nEdit this Page\nContainer Orchestration\nTable of Contents\nIntroduction\nDocker Compose\nPrerequisites\nDocker Compose Examples\nKubernetes and Helm\nKubernetes\nHelm\nPrerequisites\nUsing Our Helm Charts with Infinite Scale\nSupported Infinite Scale Versions\nBreaking Changes\nSupported Kubernetes Versions\nGet the Chart\nStart minikube\nDeploy the Chart\nWhen using K3S\nCustomize the Generic Setup\nSet Your Own Default Values\nEnable Metrics with Prometheus\nConfigure Email Notification\nConfigure S3ng Storage\nConfigure Userlog Global Notifications Secret\nDefine Mandatory Secrets and ConfigMaps\nBuilt-in User Management Secrets\nExternal User Management Secrets\nNGINX Ingress Example\nApply Chart Changes\nAccess Infinite Scale in Your Browser\nUninstalling the Chart\nIntroduction\nThis document provides guidelines for container orchestration tools suitable for Infinite Scale. These are\nDocker-Compose\nand\nKubernetes\n. Other container orchestration tools like\nDocker Swarm\ncan be derived from these. Container orchestration tools are necessary to meet the requirements described in the\nAvailability and Scalability\nguide starting at the\nContainer\nsection. The pages\nDocker Swarm vs Kubernetes: how to choose a container orchestration tool\nand\nKubernetes Vs. Docker Swarm: A Comparison of Containerization Platforms\ngive a brief overview of features and characteristics of both tools. Docker Compose\nSimilar to using\ndocker run\nand handing over command-line parameters for a single container, you can use a\ndocker-compose.yml\nyaml file which defines all the settings and environment variables for each container in one or more files. This is the next step when having multi-container environments. Consider that when planning to run Infinite Scale via Docker Compose, the degree of freedom is not the same as when using Kubernetes with Helm as you are limited to the same server and other limitations but it usually offers more than running the pure binary when it comes to ease of configuration and maintaining your environment. Use Docker Compose if you aim at an extended degree of freedom compared to the binary installation but don’t need the full capabilities of Kubernetes and Helm. Prerequisites\nCheck if the package\ndocker-compose\nis installed in addition to docker:\nwhich docker-compose\nIf\ndocker-compose\nis installed, you’ll be informed. In that case you need to install\ndocker-compose\nfirst. Version 2.x (go-based) is available from Docker. Follow the\nInstall Docker Compose\nguide to install a 2.x version. One major difference is, that compose is now a plugin of docker, even if there is still a standalone version. If the check above fails, you might also want to test:\ndocker compose version\nWhen done, create a project directory like\nocis-compose\nin your home directory to have a common location for your Infinite Scale Compose files. Docker Compose Examples\nSee the\nservices section\nfor details about available environment variables and yaml files. See below for example service configurations using Docker Compose, to get a first impression of how this can be achieved. 7.0.0\nDocker Compose deployment examples directory\nUsing git version name:\nv7.0.0\nNote that github will not let you download a single directory easily. For stable releases, check or update the version accordingly like when there is a new patch release available:\nUsing a shallow git clone which minimizes the required clone space like:\ngit clone --depth 1 https://github.com/owncloud/ocis.git -b 7.0.0\nUsing an external page providing you the folder to download as zip file like:\nhttps://download-directory.github.io?url=https://github.com/owncloud/ocis/tree/v7.0.0/deployments/examples\nKubernetes and Helm\nThe commands and examples are based on software from Kubernetes and Helm. For other software products or environments without claim to completeness like:\nOpenShift\nRancher\nK3s\nAWS EKS\nGCP GKE\nAzure AKS\nyou may need to adapt the commands and possibly the provided yaml files. Note that this does not affect the supported Kubernetes versions. Kubernetes\nKubernetes\n(abbreviated as K8s) is an open-source platform for governing clusters of containerized application services. Kubernetes automates the vital aspects of container lifecycle management, including scaling, replication, monitoring, and scheduling. Infinite Scale was designed with Kubernetes in mind. See the\nDeployment Evolution\ndescription in the\nAvailability and Scalability\nsection for reasons to use Kubernetes. When designing your Kubernetes cluster, you have two major approaches available which are\nminikube\nand\nkubeadm\n. minikube\nminikube\nlets you run a\nsingle-node Kubernetes cluster locally\n. kubeadm\nkubeadm\nrequires at least two nodes\nand builds a minimum viable, production-ready Kubernetes cluster, using best practices. It also allows the container runtime to be chosen, though it has Docker by default. A tool to note: kubectl\nkubectl\nis the command-line tool for Kubernetes. It allows users to run commands against a K8s cluster. Pods\nA\nPod\nis a Kubernetes abstraction that represents a group of one or more application containers such as Docker and some shared resources for those containers. Helm\nHelm\nis a Kubernetes deployment tool for automating creation, packaging, configuration and deployment of applications and services to Kubernetes clusters. Comparing Kubernetes to the operating system, Helm would be the package manager. Helm automates the maintenance of YAML manifests for Kubernetes objects. The image below shows the interaction of Helm v3 with Kubernetes. Prerequisites\nInstalling Kubernetes\nDepending on whether you want to go for a single or multi-node Kubernetes environment, follow the\nKubernetes Installation Documentation\nto do so. Search for comments with\nKubernetes\nfor details."
        },
        {
            "cluster": 0,
            "chunk_content": "For Kubernetes, ownCloud provides basic\nHelm Charts\nthat can be used and adjusted. Therefore ownCloud provides Helm charts for a convenient deployment of Infinite Scale on a Kubernetes cluster. This is done by packaging information into charts — therefore Helm Charts — and advertising them to a Kubernetes cluster. Verify your installation by typing:\nhelm version\nUsing Our Helm Charts with Infinite Scale\nThe Helm chart is still in an experimental phase and has not yet been published on a Helm chart repository. For your convenience, ownCloud provides an\nocis-charts\ngit repository. ownCloud will publish updated data when new Helm chart releases become available. Note that two Helm chart stable versions will be documented beside the development version. When defining your own Helm charts, consider that, if you’re using config overrides in your yaml definitions, a service does not get redefined in the override again. Multiple definitions can cause chart issues that are hard to identify. Supported Infinite Scale Versions\nSee the following table to match the Helm chart versions with Infinite Scale releases. Note that the chart version matches the tag in the\nocis-charts\ngit repository. Helm Chart Version\nWorks with Infinite Scale Versions\nlatest\n4.0.1\n0.5.0\n4.0.1\n0.4.0\n3.0.0\nIf a chart has been superseded by another for the same Infinite Scale release, only the latest one is listed. Note that Helm Chart Version\n0.2.0\nwas a necessary intermediate for Infinite Scale\n3.0.0-alpha.1\nonly and is therefore not listed with a working Infinite Scale version. Breaking Changes\nSelect possible breaking changes of a Helm chart version from the tabs. 0.5.0\n0.4.0\nBreaking Changes\nBreaking Changes\nSupported Kubernetes Versions\nlatest\n0.5.0\n0.4.0\nWe only list non EOL versions as of the chart release date from here:\nhttps://kubernetes.io/releases/\nNote that some EOL versions still might be API compatible. Version\nCovered by tests\nTested by developers\n1.28\nyes\nyes\n1.27\nyes\nyes\n1.26\nyes\nno\n1.25\nyes\nno\nWe only list non EOL versions as of the chart release date from here:\nhttps://kubernetes.io/releases/\nNote that some EOL versions still might be API compatible. Version\nCovered by tests\nTested by developers\n1.27\nyes\nyes\n1.26\nyes\nyes\n1.25\nyes\nno\n1.24\nyes\nno\nCheck the supported Kubernetes versions before you download the chart. Version\n~1.27.0-0\n~1.26.0-0\n~1.25.0-0\n~1.24.0-0\nGet the Chart\nAs the Helm chart has currently not been published to a Helm repository, you need to clone ownCloud’s Helm chart git repository named\nocis-charts\n. Deploy the chart with the deployment name\nocis\n, use any name as desired. To do so, run the following command from the root of the cloned repository:\nhelm install ocis ./charts/ocis\nlatest\n0.5.0\n0.4.0\nFile\nDescription\nvalues.yaml\nHelm chart with default configurations. File\nDescription\nvalues.yaml\nHelm chart with default configurations. File\nDescription\nvalues.yaml\nHelm chart with default configurations. If you’re using Helm Charts, you are responsible for these user management secrets and their lifecycle."
        },
        {
            "cluster": 3,
            "chunk_content": "ownCloud highly recommends reading the\nGeneral Info\nas it contains valuable information about configuration rules, managing services and default paths - just to mention some of the useful topics. If not, you may get no output at all or a message that it couldn’t be found. On most Linux distributions, you can simply use the package manager to do so. Note that in many cases, this will install a 1.x version (python-based) only supported until June 2023. Familiarize yourself with the\ndifferences between versions\nparticularly with regard to syntax changes. Most of our examples are still based on Compose-V1. Both\nocis environment variables\nand\nocis service configuration\nyaml files are used. You can get the examples using the following methods requiring minimum space. Check your software or environment for details and requirements. Any software or environment must match the version standards. It offers a framework for distributed systems. Infinite Scale follows the\nTwelve-Factor App\nprinciples regarding configuration, which means almost every aspect of Infinite Scale is modifiable via environment variables. It is a good way to test a deployment. It requires no extra configuration on any cloud platform as everything runs on your local machine. It supports multiple contexts for as many clusters as you have access to. This information will be available in additional tabs in corresponding sections. The\nvalues.yaml\nfile provided by ownCloud uses generic configuration. You can customize this configuration with your own values, for example for different setups or sizings. This should be done by using your own\nvalues.yaml\nfile at a different location which will\noverwrite\nor\nadd\ncontent to the provided one. While not mandatory, the identical file name of\nvalues.yaml\nfollows the convention of Helm. A new document with all details will be opened, directly referring to the selected version, although the document contains information about all published versions. The\n~\nrepresents all patch releases for that particular version. The\n-0\nrepresents subversions of that particular version. Values Description\nDescription of the values.yaml file. Values Description\nDescription of the values.yaml file. Values Description\nDescription of the values.yaml file. Customize the Generic Setup\nIn all examples, adapt the settings according your needs. In order to apply the ServiceMonitor, you need to have Prometheus' CustomResourceDefinitions available, e.g. by installing the\nPrometheus Operator\n. latest\n0.5.0\n0.4.0\nextraResources:\n- |\napiVersion: monitoring.coreos.com/v1\nkind: ServiceMonitor\nmetadata:\nname: ocis-metrics\nspec:\nselector:\nmatchLabels:\nocis-metrics: enabled\nendpoints:\n- port: metrics-debug\ninterval: 60s\nscrapeTimeout: 30s\npath: /metrics\nextraResources:\n- |\napiVersion: monitoring.coreos.com/v1\nkind: ServiceMonitor\nmetadata:\nname: ocis-metrics\nspec:\nselector:\nmatchLabels:\nocis-metrics: enabled\nendpoints:\n- port: metrics-debug\ninterval: 60s\nscrapeTimeout: 30s\npath: /metrics\nextraResources:\n- |\napiVersion: monitoring.coreos.com/v1\nkind: ServiceMonitor\nmetadata:\nname: ocis-metrics\nspec:\nselector:\nmatchLabels:\nocis-metrics: enabled\nendpoints:\n- port: metrics-debug\ninterval: 60s\nscrapeTimeout: 30s\npath: /metrics\nConfigure Email Notification\nIf the key\nfeatures.emailNotifications.enable\nis set to\ntrue\n, the SMTP email server Secret referenced in\nsecretRefs.notificationsSmtpSecretRef\nneeds to be configured:\nlatest\n0.5.0\n0.4.0\n---\napiVersion: v1\nkind: Secret\nmetadata:\nname: notifications-smtp-secret # needs to be set to `secretRefs.notificationsSmtpSecretRef`\ntype: Opaque\ndata:\n# Username for authentication against the SMTP host. They are created\none-off\nif you don’t explicitly provide them. Proper yaml formatting is necessary. Mandatory ConfigMaps\nIf you want to manage ConfigMaps on your own, you can look at the following example which shows how mandatory ConfigMaps look like and how they can be generated. The example assumes that the\nconfigRefs\nare not changed. Each ConfigMaps data entry holds a description of how to generate it or find the right value. The following example shows what generic configuration need to look like and how they can be generated. The example assumes that the\nconfigRefs\nare not changed . Each config data entry holds a description of how to generate it or find the right value. latest\n0.5.0\n0.4.0\nFile\nDescription\ngeneric-configs.yaml\nList of all mandatory ConfigMaps. File\nDescription\ngeneric-configs.yaml\nList of all mandatory ConfigMaps. File\nDescription\ngeneric-configs.yaml\nList of all mandatory ConfigMaps. Apply Mandatory ConfigMaps\nConfigMaps can be applied by command or included in\nextraResources\nof your own\nvalues.yaml\nfile. Proper yaml formatting is necessary. These are also autogenerated for you if you don’t provide them manually. Rotation can be achieved by deleting the Secrets\nldap-ca\nand\nldap-cert\nand restarting all Infinite Scale deployments like with\nkubectl rollout restart deploy\n. Proper yaml formatting is necessary. Certificates are also required which should expire and therefore need a certificate rotation from time to time, for which we didn’t document appropiate tooling yet. Proper yaml formatting is necessary. The ocis init Command\nOffice Applications using WOPI\nAbout ownCloud\nThe Secure Collaboration Platform\nNews\nPrivacy statement\nImprint\nResources\nFAQ\nHelp\nSecurity\nChangelog\nGet started\nInteract\nRocket.Chat\nForum\nContribute\nCustomer support\n© Copyright 2011-2025, The ownCloud developers."
        },
        {
            "cluster": 1,
            "chunk_content": "As an example, OpenShift requires, if not otherwise defined, the container user and password ID to be a very high number. When it comes to security sensitive data like secrets, such data is usually\nnot\nadded in the overwrite-values file for security reasons. In such a case, you apply secrets via command from a secrets file. smtp-password: XXXXXXXXXXXXX\n---\napiVersion: v1\nkind: Secret\nmetadata:\nname: notifications-smtp-secret # needs to be set to `secretRefs.notificationsSmtpSecretRef`\ntype: Opaque\ndata:\n# Username for authentication against the SMTP host. smtp-password: XXXXXXXXXXXXX\n---\napiVersion: v1\nkind: Secret\nmetadata:\nname: notifications-smtp-secret # needs to be set to `secretRefs.notificationsSmtpSecretRef`\ntype: Opaque\ndata:\n# Username for authentication against the SMTP host. smtp-password: XXXXXXXXXXXXX\nConfigure S3ng Storage\nIf the key\nservices.storageusers.storageBackend.driver\nis set to\ns3ng\n, the S3 access key ID / secret Secret referenced in\nsecretRefs.s3CredentialsSecretRef\nneeds to be configured:\nlatest\n0.5.0\n0.4.0\n---\napiVersion: v1\nkind: Secret\nmetadata:\nname: s3-credentials-secret # default of `secretRefs.s3CredentialsSecretRef`\ntype: Opaque\ndata:\n# S3 access key. accessKey: XXXXXXXXXXXXX\n# S3 secret key. secretKey: XXXXXXXXXXXXX\n---\napiVersion: v1\nkind: Secret\nmetadata:\nname: s3-credentials-secret # default of `secretRefs.s3CredentialsSecretRef`\ntype: Opaque\ndata:\n# S3 access key. accessKey: XXXXXXXXXXXXX\n# S3 secret key. secretKey: XXXXXXXXXXXXX\n---\napiVersion: v1\nkind: Secret\nmetadata:\nname: s3-credentials-secret # default of `secretRefs.s3CredentialsSecretRef`\ntype: Opaque\ndata:\n# S3 access key. accessKey: XXXXXXXXXXXXX\n# S3 secret key. secretKey: XXXXXXXXXXXXX\nConfigure Userlog Global Notifications Secret\nConfigure\nGlobal Notifications Secrets\nreferenced in\nsecretRefs.globalNotificationsSecretRef\nif required:\nlatest\n---\napiVersion: v1\nkind: Secret\nmetadata:\nnamespace: ocis\nname: userlog-notifications-secret # default of `secretRefs.globalNotificationsSecretRef`\ntype: Opaque\ndata:\n# token\nnotifications-secret: XXXXXXXXXXXXX\nDefine Mandatory Secrets and ConfigMaps\nInfinite Scale requires some mandatory Secrets and ConfigMaps to work. If you’re using the builtin user management, which is not recommended, among the auto generated Secrets, there are also some certificates which expire and need to be renewed manually. These Secrets and ConfigMaps need to be part of your backup since you need to provide them manually during a disaster recovery procedure. Mandatory Secrets\nIf you want to manage Secrets on your own, you can look at the following example which shows what mandatory Secrets look like and how they can be generated. The example assumes that the\nsecretRefs\nare not changed. Each Secret data entry holds a description of how to generate it or find the right value. latest\n0.5.0\n0.4.0\nFile\nDescription\ngeneric-secrets.yaml\nList of all mandatory Secrets. File\nDescription\ngeneric-secrets.yaml\nList of all mandatory Secrets. File\nDescription\ngeneric-secrets.yaml\nList of all mandatory Secrets. Apply Mandatory Secrets\nSecrets can be applied by command or included in\nextraResources\nof your own\nvalues.yaml\nfile. Adapt the data content according to your environment:\nTo apply secrets by command, save the content as\nmandatory-secrets.yaml\nand use the following command with a path to the secrets file added if necessary:\nkubectl apply -f mandatory-secrets.yaml\nTo apply secrets via your own\nvalues.yaml\n, add the content at\nextraResources\n. Adapt the data content according to your environment:\nTo apply configs by command, save the content as\ngeneric-configs.yaml\nand use the following command with a path to the secrets file added if necessary:\nkubectl apply -f generic-configs.yaml\nTo apply configs via your own\nvalues.yaml\n, add the content at\nextraResources\n. Built-in User Management Secrets\nIf you’re using the built-in user management by setting\nfeatures.externalUserManagement.enabled\nto\nfalse\n, which is the default, you’ll need additional Secrets. These Secrets are certificates that expire after 365 days and therefore need a certificate rotation from time to time. The following example shows what the Secrets for the built-in user management need to look like and how they can be generated. The example assumes that the\nsecretRefs\nare not changed . Each Secret data entry holds a description of how to generate it or find the right value. latest\n0.5.0\n0.4.0\nFile\nDescription\nbuiltin-user-mgmt-secrets.yaml\nSecrets file for the builtin user management. File\nDescription\nbuiltin-user-mgmt-secrets.yaml\nSecrets file for the builtin user management. File\nDescription\nbuiltin-user-mgmt-secrets.yaml\nSecrets file for the builtin user management. Apply Built-in User Management Secrets\nSecrets can be applied by command or included in\nextraResources\nof your own\nvalues.yaml\nfile. Adapt the data content according to your environment:\nTo apply secrets by command, save the content as\nbuiltin-user-mgmt-secrets.yaml\nand use the following command (adding the path to the secrets file if necessary):\nkubectl apply -f builtin-user-mgmt-secrets.yaml\nTo apply secrets via your own\nvalues.yaml\n, add the content at\nextraResources\n. External User Management Secrets\nIf you’re using external user management by setting\nfeatures.externalUserManagement.enabled\nto\ntrue\n, you need to set these Secrets. Any information necessary to use this security-relevant data is provided by ownCloud via examples. The following example shows what external user management secrets need to look like and how they can be generated. The example assumes that the\nsecretRefs\nare not changed . Each secret data entry holds a description of how to generate it or find the right value. latest\n0.5.0\n0.4.0\nFile\nDescription\nexternal-user-mgmt-secrets.yaml\nSecrets file for the external user management. File\nDescription\nexternal-user-mgmt-secrets.yaml\nSecrets file for the external user management. File\nDescription\nexternal-user-mgmt-secrets.yaml\nSecrets file for the external user management. Apply External User Management Secrets\nSecrets can be applied by command or included in\nextraResources\nof your own\nvalues.yaml\nfile. Adapt the data content according to your environment:\nTo apply secrets by command, save the content as\nexternal-user-mgmt-secrets.yaml\nand use the following command (adding a path to the secrets file if necessary):\nkubectl apply -f external-user-mgmt-secrets.yaml\nTo apply secrets via your own\nvalues.yaml\n, add the content at\nextraResources\n."
        },
        {
            "cluster": 4,
            "chunk_content": "minikube\nalso provides kubectl wrapped as\nminikube kubectcl\n. This documentation will use minikube in the examples. Verify your installation by typing:\nminikube version\nInstalling Helm\nFollow the\nHelm Installation Documentation\npost installation and setup of Kubernetes. Start minikube\nStart your minikube cluster with the latest supported K8s version:\nminikube start --kubernetes-version=v1.28.1\nEnable the\nminikube ingress\nplugin, which acts like a reverse proxy for your cluster:\nminikube addons enable ingress\nLinux only: enable the\ningress-dns\nplugin and configure it:\nminikube addons enable ingress-dns\nConfigure the\nin-cluster DNS server\nto resolve local DNS names inside the cluster`:\nNote that this step is not optional but mandatory for an Infinite Scale installation. For details see\nStep 4, (optional) Configure in-cluster DNS server to resolve local DNS names inside cluster\n. macOS only: run\nminikube tunnel\nto be expose\n80,443\nports:\nminikube tunnel\nConfigure hosts:\nOn Linux you need to add additional configurations to use ingress by adding the domain names to\n/etc/hosts\n. Those entries need to point to the Minikube interface IP which you can get by running\nminikube ip\n. 192.168.49.2 ocis.kube.owncloud.test\nOn macOS you need to add additional configurations to use ingress by adding the domain names to\n/etc/hosts\n. Since you are using\nminikube tunnel\n, those entries need to point to\n127.0.0.1\nbecause it’s listening on the localhost interface. 127.0.0.1 ocis.kube.owncloud.test\nDeploy the Chart\nAdd accessible oCIS domain to\nexternalDomain\nin\nvalues.yaml\nexternalDomain: \"ocis.kube.owncloud.test\"\nWhen installing Infinite Scale in Minikube on MacOS, you need to set the\nhostAliases\noption:\nhostAliases:\n- ip: \"192.168.49.2\" # <- needs to be the IP of `minikube ip`\nhostnames:\n- \"ocis.kube.owncloud.test\"\nBased on the Kubernetes version, you will find comments in\nvalues.yaml\nwhere content depends on the Kubernetes version. When using K3S\nCoreDNS needs to know how to resolve domains like\nocis.kube.owncloud.test\nto the host, like\nraspbian-bullseye-arm64\n. To authenticate users via OIDC, the proxy service has to be able to resolve the\nexternalDomain:ocis.kube.owncloud.test\n. Since all\n*.kube.owncloud.test\ndomains should point to the host, the CoreDNS rewrite plugin can be used like the following:\napiVersion: v1\nkind: ConfigMap\nmetadata:\nname: coredns-custom\nnamespace: kube-system\ndata:\nrewritehost.override: |\nrewrite name regex (. *).kube.owncloud.test raspbian-bullseye-arm64\nReplace:\nraspbian-bullseye-arm64\nwith your hostname and\n(. *).kube.owncloud.test\nwith a regex for the domains you want to point back to the host. Apply the config and restart CoreDNS:\nkubectl apply -f corednsms.yaml\nkubectl -n kube-system rollout restart deployment coredns\nFinally, test that resolving the domain works:\nCommand\nkubectl -n ocis get pods | grep proxy\nOutput\nproxy-76bdf4bdb6-j5rmp               1/1     Running   0             25h\n$ kubectl -n ocis exec proxy-76bdf4bdb6-j5rmp -- ping ocis.kube.owncloud.test\nPING ocis.kube.owncloud.test (192.168.1.208): 56 data bytes\n64 bytes from 192.168.1.208: seq=0 ttl=42 time=0.243 ms\n64 bytes from 192.168.1.208: seq=1 ttl=42 time=0.359 ms\nSee the following links for more information:\nK3S CoreDNS config\nand\nMS Docs on how to customize CoreDNS\n. Set Your Own Default Values\nCreate your\nown local\nvalues.yaml\nfile which will overwrite parts of the provided one with the following content:\nlatest\n0.5.0\n0.4.0\nexternalDomain: ocis.kube.owncloud.test\ningress:\nenabled: true\nannotations:\nnginx.ingress.kubernetes.io/proxy-body-size: 1024m\ntls:\n- hosts:\n- ocis.kube.owncloud.test\ninsecure:\n# disables ssl certificate checking for connections to the openID connect identity provider. # Not recommended for production setups, but we don't have valid certificates in minikube\noidcIdpInsecure: true\n# disables ssl certificate checking for connections to the oCIS http apis. # Not recommended for production setups, but we don't have valid certificates in minikube\nocisHttpApiInsecure: true\nexternalDomain: ocis.kube.owncloud.test\ningress:\nenabled: true\nannotations:\nnginx.ingress.kubernetes.io/proxy-body-size: 1024m\ntls:\n- hosts:\n- ocis.kube.owncloud.test\ninsecure:\n# disables ssl certificate checking for connections to the openID connect identity provider. # Not recommended for production setups, but we don't have valid certificates in minikube\noidcIdpInsecure: true\n# disables ssl certificate checking for connections to the oCIS http apis. # Not recommended for production setups, but we don't have valid certificates in minikube\nocisHttpApiInsecure: true\nexternalDomain: ocis.kube.owncloud.test\ningress:\nenabled: true\nannotations:\nnginx.ingress.kubernetes.io/proxy-body-size: 1024m\ntls:\n- hosts:\n- ocis.kube.owncloud.test\ninsecure:\n# disables ssl certificate checking for connections to the openID connect identity provider. # Not recommended for production setups, but we don't have valid certificates in minikube\noidcIdpInsecure: true\n# disables ssl certificate checking for connections to the oCIS http apis. # Not recommended for production setups, but we don't have valid certificates in minikube\nocisHttpApiInsecure: true\nEnable Metrics with Prometheus\nIn order to scrape oCIS' metrics with Prometheus, you need to set up a\nServiceMonitor\n. smtp-username: XXXXXXXXXXXXX\n# Password for authentication against the the SMTP host. smtp-username: XXXXXXXXXXXXX\n# Password for authentication against the the SMTP host. smtp-username: XXXXXXXXXXXXX\n# Password for authentication against the the SMTP host. NGINX Ingress Example\nThis is an example with NGINX ingress and certificates issued by cert-manager. To make this work, you need to have NGINX ingress and\ncert-manager\ninstalled in your cluster. Defining NGINX ingress and cert-manager. latest\n0.5.0\n0.4.0\nexternalDomain: ocis.owncloud.test\ningress:\nenabled: true\ningressClassName: nginx\nannotations:\ncert-manager.io/issuer: 'ocis-certificate-issuer'\ntls:\n- hosts:\n- ocis.kube.owncloud.test\nsecretName: ocis-tls-certificate\nextraResources:\n- |\napiVersion: cert-manager.io/v1\nkind: Issuer\nmetadata:\nname: ocis-certificate-issuer\nspec:\nacme:\nserver: https://acme-v02.api.letsencrypt.org/directory\nemail: test@example.test\nprivateKeySecretRef:\nname: ocis-certificate-issuer\nsolvers:\n- http01:\ningress:\nclass: nginx\nexternalDomain: ocis.owncloud.test\ningress:\nenabled: true\ningressClassName: nginx\nannotations:\ncert-manager.io/issuer: 'ocis-certificate-issuer'\ntls:\n- hosts:\n- ocis.kube.owncloud.test\nsecretName: ocis-tls-certificate\nextraResources:\n- |\napiVersion: cert-manager.io/v1\nkind: Issuer\nmetadata:\nname: ocis-certificate-issuer\nspec:\nacme:\nserver: https://acme-v02.api.letsencrypt.org/directory\nemail: test@example.test\nprivateKeySecretRef:\nname: ocis-certificate-issuer\nsolvers:\n- http01:\ningress:\nclass: nginx\nexternalDomain: ocis.owncloud.test\ningress:\nenabled: true\ningressClassName: nginx\nannotations:\ncert-manager.io/issuer: 'ocis-certificate-issuer'\ntls:\n- hosts:\n- ocis.kube.owncloud.test\nsecretName: ocis-tls-certificate\nextraResources:\n- |\napiVersion: cert-manager.io/v1\nkind: Issuer\nmetadata:\nname: ocis-certificate-issuer\nspec:\nacme:\nserver: https://acme-v02.api.letsencrypt.org/directory\nemail: test@example.test\nprivateKeySecretRef:\nname: ocis-certificate-issuer\nsolvers:\n- http01:\ningress:\nclass: nginx\nApply Chart Changes\nApply all changes defined in your\nown\nvalues.yaml\nfile:\nhelm upgrade --install --reset-values \\\nocis ./charts/ocis --values values.yaml\nEnsure that all the pods are running:\nkubectl get pods\nAccess Infinite Scale in Your Browser\nAfter you have customized your setup, use the following URL to access Infinite Scale with your browser:\nhttps://ocis.kube.owncloud.test\nUninstalling the Chart\nTo uninstall/delete the\nocis\ndeployment, use the following command:\nhelm delete ocis\nThis command removes all the Kubernetes components associated with the chart and deletes the deployment."
        }
    ]
}